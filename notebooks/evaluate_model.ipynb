{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a96689-1ff6-41a7-b0c6-18cb0b439357",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_eos4tcc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m predictions_eos43at \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/eos4tcc/prediction_eos4tcc.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Calculate and display metrics for Model eos4tcc\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m calculate_metrics_and_display(\u001b[43mpredictions_eos4tcc\u001b[49m, true_labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel eos4tcc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_eos4tcc' is not defined"
     ]
    }
   ],
   "source": [
    "###Model eos4tcc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Function to calculate and display performance metrics\n",
    "def calculate_metrics_and_display(predictions, true_labels, model_name):\n",
    "    # Assuming 'score' column represents predictions for class 1 (positive class)\n",
    "    predictions_positive_class = predictions['score']\n",
    "\n",
    "    # Convert probabilities to binary predictions (use a threshold of 0.5 for simplicity)\n",
    "    binary_predictions = (predictions_positive_class >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    precision = precision_score(true_labels, binary_predictions)\n",
    "    recall = recall_score(true_labels, binary_predictions)\n",
    "    f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix_df)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predictions_positive_class)\n",
    "    auc = roc_auc_score(true_labels, predictions_positive_class)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name}, AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Load testing dataset\n",
    "testing_data = pd.read_csv('../data/updated_test_dataset.csv')\n",
    "\n",
    "# Extract features and true labels\n",
    "testing_features = testing_data.drop(columns=['activity'])\n",
    "true_labels = testing_data['activity']\n",
    "\n",
    "# Load predictions for Model eos4tcc (replace with actual file path)\n",
    "predictions_eos4tcc = pd.read_csv('../data/eos4tcc/prediction_eos4tcc.csv')\n",
    "\n",
    "# Calculate and display metrics for Model eos4tcc\n",
    "calculate_metrics_and_display(predictions_eos4tcc, true_labels, 'Model eos4tcc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf9955-1d6e-4c46-9116-3a01f4f6a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meaning of the metrics\n",
    "\n",
    "# Accuracy: The proportion of correctly classified instances out of the total instances. In this case, it's 82%, indicating that 82% of the instances were classified correctly.\n",
    "\n",
    "# Precision: The proportion of true positive predictions out of the total predicted positives. In this case, it's 100%, meaning that when the model predicts a positive class, it is correct 100% of the time.\n",
    "\n",
    "# Recall (Sensitivity): The proportion of true positive predictions out of the total actual positives. In this case, it's 64%, meaning that the model captures 64% of the actual positive instances.\n",
    "\n",
    "# F1-score: The harmonic mean of precision and recall. It provides a balance between precision and recall. In this case, it's 78.05%.\n",
    "\n",
    "#  interpreting the confusion matrix:\n",
    "\n",
    "# True Positive (TP): 32 instances were correctly predicted as positive.\n",
    "# True Negative (TN): 50 instances were correctly predicted as negative.\n",
    "# False Positive (FP): 0 instances were wrongly predicted as positive.\n",
    "# False Negative (FN): 18 instances were wrongly predicted as negative.\n",
    "# So, the confusion matrix shows that the model is performing well in terms of accuracy and precision. However, it could potentially improve its recall (capturing more of the actual positive instances). Depending on the specific goals and requirements of your application, you might want to fine-tune the model to achieve a better balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a8e00-acbe-4436-8238-35598f372769",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Model eos43at\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Function to calculate and display performance metrics\n",
    "def calculate_metrics_and_display(predictions, true_labels, model_name):\n",
    "    # Assuming 'score' column represents predictions for class 1 (positive class)\n",
    "    predictions_positive_class = predictions['pic50']\n",
    "\n",
    "    # Convert probabilities to binary predictions (use a threshold of 0.5 for simplicity)\n",
    "    binary_predictions = (predictions_positive_class >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    precision = precision_score(true_labels, binary_predictions)\n",
    "    recall = recall_score(true_labels, binary_predictions)\n",
    "    f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix_df)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predictions_positive_class)\n",
    "    auc = roc_auc_score(true_labels, predictions_positive_class)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name}, AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Load testing dataset\n",
    "testing_data = pd.read_csv('../data/updated_test_dataset.csv')\n",
    "\n",
    "# Extract features and true labels\n",
    "testing_features = testing_data.drop(columns=['activity'])\n",
    "true_labels = testing_data['activity']\n",
    "\n",
    "# Load predictions for Model eos4tcc (replace with actual file path)\n",
    "predictions_eos43at = pd.read_csv('../data/eos43at/prediction_eos43at.csv')\n",
    "\n",
    "# Calculate and display metrics for Model eos4tcc\n",
    "calculate_metrics_and_display(predictions_eos43at, true_labels, 'Model eos43at')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad00bd4-b92a-44c6-af97-d6a4f6a3886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model eos2ta5\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Function to calculate and display performance metrics\n",
    "def calculate_metrics_and_display(predictions, true_labels, model_name):\n",
    "    # Assuming 'probability' column represents predictions for class 1 (positive class)\n",
    "    predictions_positive_class = predictions['probability']\n",
    "\n",
    "    # Handling NaN values by replacing them with 0.5 (you can choose a different strategy)\n",
    "    predictions_positive_class = np.nan_to_num(predictions_positive_class, nan=0.5)\n",
    "\n",
    "    # Convert probabilities to binary predictions (use a threshold of 0.5 for simplicity)\n",
    "    binary_predictions = (predictions_positive_class >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    precision = precision_score(true_labels, binary_predictions)\n",
    "    recall = recall_score(true_labels, binary_predictions)\n",
    "    f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix_df)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predictions_positive_class)\n",
    "    auc = roc_auc_score(true_labels, predictions_positive_class)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name}, AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Load testing dataset\n",
    "testing_data = pd.read_csv('../data/updated_test_dataset.csv')\n",
    "\n",
    "# Extract features and true labels\n",
    "testing_features = testing_data.drop(columns=['activity'])\n",
    "true_labels = testing_data['activity']\n",
    "\n",
    "# Load predictions for Model eos2ta5 (replace with actual file path)\n",
    "predictions_eos2ta5 = pd.read_csv('../data/eos2ta5/prediction_eos2ta5.csv')\n",
    "\n",
    "# Calculate and display metrics for Model eos2ta5\n",
    "calculate_metrics_and_display(predictions_eos2ta5, true_labels, 'Model eos2ta5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8da5e-178e-418f-b127-b01c8f6caee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Model eos30gr\n",
    "\n",
    "# Function to calculate and display performance metrics\n",
    "def calculate_metrics_and_display(predictions, true_labels, model_name):\n",
    "    # Assuming 'activity10' column represents predictions for class 1 (positive class)\n",
    "    predictions_positive_class = predictions['activity10']\n",
    "\n",
    "    # Drop NaN values from both predictions and true labels\n",
    "    non_nan_mask = ~predictions_positive_class.isna() & ~true_labels.isna()\n",
    "    predictions_positive_class = predictions_positive_class[non_nan_mask]\n",
    "    true_labels = true_labels[non_nan_mask]\n",
    "\n",
    "    # Convert probabilities to binary predictions (use a threshold of 0.5 for simplicity)\n",
    "    binary_predictions = (predictions_positive_class >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    precision = precision_score(true_labels, binary_predictions)\n",
    "    recall = recall_score(true_labels, binary_predictions)\n",
    "    f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix_df)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predictions_positive_class)\n",
    "    auc = roc_auc_score(true_labels, predictions_positive_class)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name}, AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Load testing dataset\n",
    "testing_data = pd.read_csv('../data/updated_test_dataset.csv')\n",
    "\n",
    "# Extract features and true labels\n",
    "testing_features = testing_data.drop(columns=['activity'])\n",
    "true_labels = testing_data['activity']\n",
    "\n",
    "# Load predictions for Model eos30gr (replace with actual file path)\n",
    "predictions_eos30gr = pd.read_csv('../data/eos30gr/prediction_eos30gr.csv')\n",
    "\n",
    "# Calculate and display metrics for Model eos30gr\n",
    "calculate_metrics_and_display(predictions_eos30gr, true_labels, 'Model eos30gr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421d55d-101e-429a-b7fe-1fbf5502df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Model eos30f3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Function to calculate and display performance metrics\n",
    "def calculate_metrics_and_display(predictions, true_labels, model_name):\n",
    "    # Assuming 'score' column represents predictions for class 1 (positive class)\n",
    "    predictions_positive_class = predictions['activity']\n",
    "\n",
    "    # Convert probabilities to binary predictions (use a threshold of 0.5 for simplicity)\n",
    "    binary_predictions = (predictions_positive_class >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    precision = precision_score(true_labels, binary_predictions)\n",
    "    recall = recall_score(true_labels, binary_predictions)\n",
    "    f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix_df)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predictions_positive_class)\n",
    "    auc = roc_auc_score(true_labels, predictions_positive_class)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name}, AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Load testing dataset\n",
    "testing_data = pd.read_csv('../data/updated_test_dataset.csv')\n",
    "\n",
    "# Extract features and true labels\n",
    "testing_features = testing_data.drop(columns=['activity'])\n",
    "true_labels = testing_data['activity']\n",
    "\n",
    "# Load predictions for Model eos4tcc (replace with actual file path)\n",
    "predictions_eos30f3 = pd.read_csv('../data/eos30f3/prediction_eos30f3.csv')\n",
    "\n",
    "# Calculate and display metrics for Model eos4tcc\n",
    "calculate_metrics_and_display(predictions_eos30f3, true_labels, 'Model eos30f3')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc4d18-f379-4ef2-83b2-2068406f61dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
